# 03 — AI Usability Guidelines (2025-2026 Standard)

As AI becomes deeply integrated into digital products, new design challenges emerge. This document provides the comprehensive framework for designing human-AI interactions that are transparent, controllable, and trustworthy.

---

## Core Principle: AI as Copilot, Not Autopilot

> AI should **augment** human decision-making, not replace it. The user must always feel in control, informed, and empowered to override.

### The Human-in-the-Loop Spectrum

```
Full Manual ←→ AI Suggests ←→ AI Acts + Human Approves ←→ AI Acts + Human Can Undo ←→ Full Automation
     1              2                    3                          4                        5
```

**Design Rule:** Default to Level 2-3 for most features. Only use Level 4-5 for low-risk, reversible actions. Never use Level 5 for decisions that affect money, data deletion, or user safety.

---

## 1. System Status Visibility for AI

### Visual Distinction of AI-Generated Content

**Principle:** Users must always be able to distinguish AI-generated content from human-created content or system data.

**Implementation Patterns:**

| Pattern | Description | Use Case |
|---------|-------------|----------|
| **AI Badge/Icon** | Small "AI" or sparkle (✦) icon next to generated content | Inline suggestions, auto-completions |
| **Color Coding** | Purple/violet highlight or border | AI-generated paragraphs, code blocks |
| **Container Styling** | Distinct card style (dashed border, gradient accent) | AI recommendations, summaries |
| **Attribution Label** | "Generated by AI" footer text | Reports, analyses, email drafts |
| **Confidence Indicator** | Confidence percentage or level (High/Medium/Low) | Predictions, classifications |

**Rules:**
1. AI indicators must be **persistent** — don't fade them out after a few seconds
2. If the user edits AI content, change the indicator to "AI-generated, edited by [user]"
3. Use consistent AI visual language across the entire product
4. Never disguise AI output as human-written content

### Processing State Communication

**Bad:** Generic spinner with "Loading..."
**Good:** Multi-step progress with specific context

**Progress Pattern:**
```
Step 1: "Analyzing your data..." (with pulsing animation)
Step 2: "Identifying patterns..."
Step 3: "Fine-tuning layout..."
Step 4: "Generating recommendations..."
Step 5: "Ready for your review" (completion state)
```

**Implementation Guidelines:**
- Show the **current step name** (not just a percentage)
- For processes >5 seconds, show estimated time remaining
- Allow cancellation at any point during processing
- If an error occurs mid-process, show what was completed and what failed
- Use skeleton/shimmer UI for the expected output area while processing

---

## 2. User Control & Freedom

### The Undo/Cancel/Feedback Triad

Every AI action must provide three control mechanisms:

#### A. Undo
- **Immediate undo** for all AI-generated changes (Ctrl/Cmd+Z or explicit undo button)
- **Version history** for documents modified by AI
- **Before/after comparison** view when AI makes significant changes
- Undo must restore the exact previous state, not an approximation

#### B. Cancel
- Cancel button visible during all AI processing
- Cancellation should be **instantaneous** (stop the UI update immediately, even if the backend is still processing)
- After cancellation, restore the pre-action state completely
- Never penalize users for cancelling (no cooldown, no "are you sure?")

#### C. Feedback
- **Thumbs up/down** on AI outputs (minimum viable feedback)
- **Reason selection** for negative feedback ("Irrelevant," "Inaccurate," "Too verbose," etc.)
- **Free-text feedback** option for detailed input
- **"Not helpful" → "Show alternatives"** flow (don't dead-end on negative feedback)
- Feedback should feel lightweight — never interrupt the workflow

### Automation Level Controls

Let users control how much AI assistance they receive:

```
Off → Suggestions Only → Auto-Apply with Review → Full Auto
```

**Per-feature granularity:** Users should be able to set automation levels for individual features, not just a global on/off.

**Example:**
- Auto-complete: ON (suggestions only)
- Auto-categorization: ON (auto-apply with review)
- Auto-scheduling: OFF
- Smart notifications: ON (full auto)

---

## 3. Explainability (XAI — Explainable AI)

### Why Explainability Matters
- Builds user trust over time
- Enables users to catch AI errors
- Supports learning (users understand patterns)
- Required by regulation in some domains (EU AI Act, GDPR Article 22)

### Explainability Patterns

#### Level 1: Input Attribution
> "This recommendation is based on **your last 30 days of activity** and **similar users' preferences**."

Show which data inputs influenced the output. Use bold/highlight to emphasize key factors.

#### Level 2: Reasoning Summary
> "We recommend Layout B because: (1) it has 23% higher click-through rate in A/B tests, (2) it follows your brand's visual hierarchy, (3) it matches mobile-first best practices."

Provide numbered, scannable reasons. Each reason should be verifiable.

#### Level 3: Confidence & Alternatives
> "Confidence: 87% — [Layout B recommended] | Also considered: Layout A (72% confidence), Layout C (65% confidence)"

Show the AI's confidence level and what alternatives were considered.

#### Level 4: Interactive Exploration
- Allow users to ask "Why not X?" and get an explanation
- Provide "What if?" tools to adjust inputs and see how recommendations change
- Visual explanation overlays (highlight which parts of an image influenced a decision)

### Explainability UX Guidelines
1. **Default:** Show Level 1 (brief attribution)
2. **On demand:** Provide Level 2-3 via expandable section or tooltip
3. **Expert mode:** Offer Level 4 for power users and auditors
4. Never force users to read explanations — make them available but not blocking
5. Use plain language, not technical jargon ("based on past data" not "trained on historical embeddings")

---

## 4. Trust Calibration

### The Trust Lifecycle

```
First Use → Skepticism → Testing → Calibration → Appropriate Trust → Ongoing Monitoring
```

**Design for each stage:**

| Stage | User Behavior | Design Strategy |
|-------|--------------|-----------------|
| **Skepticism** | "I don't trust this" | Show credentials, accuracy stats, explanations |
| **Testing** | "Let me verify this" | Make verification easy (show sources, allow manual override) |
| **Calibration** | "I'm learning when to trust this" | Show confidence levels, be transparent about limitations |
| **Appropriate Trust** | "I know when this works and when it doesn't" | Allow automation for trusted areas, keep alerts for edge cases |
| **Over-trust** (danger) | "I'll accept everything" | Periodically prompt manual review, flag low-confidence outputs |

### Preventing Over-Trust
- Periodically require manual verification for high-stakes decisions
- Rotate which items get auto-approved vs. manual review
- Show "AI was wrong" examples to maintain healthy skepticism
- Never hide low-confidence outputs — always surface them for review

### Preventing Under-Trust
- Track AI accuracy transparently ("AI suggestions were accepted 89% of the time this month")
- Celebrate AI wins ("AI caught 12 errors that would have been missed")
- Gradual trust-building: start with low-stakes features, earn trust for higher-stakes ones

---

## 5. Error Handling for AI Features

### AI-Specific Error Types

| Error Type | Example | Design Response |
|-----------|---------|----------------|
| **No output** | AI couldn't generate anything | Explain why, offer manual path |
| **Low confidence** | AI isn't sure about the result | Show confidence, suggest manual review |
| **Hallucination** | AI generated incorrect information | Encourage source verification, flag unverified claims |
| **Bias detection** | AI output may contain bias | Surface warning, offer alternative perspectives |
| **Timeout** | AI processing took too long | Auto-cancel, offer retry or manual path |
| **Partial result** | AI completed some but not all of the task | Show what was completed, let user finish manually |

### Error UX Principles
1. Never blame the user for AI failures ("Sorry, I couldn't process that" not "Invalid input")
2. Always provide an alternative path (manual mode, different approach, human support)
3. Log AI errors for improvement (with user consent)
4. If AI repeatedly fails for a specific use case, proactively suggest manual mode

---

## 6. AI Onboarding & Education

### First-Time AI Feature Introduction

**Pattern: The AI Handshake**
1. **Explain what the AI does** (one sentence, plain language)
2. **Show an example** (before/after or demo animation)
3. **Set expectations** ("This works best for X, may struggle with Y")
4. **Offer control** ("You can adjust or turn this off anytime in Settings")
5. **Let the user try** (interactive, low-stakes first use)

### Ongoing AI Education
- Contextual tips when AI features are underused
- "Did you know?" prompts for advanced AI capabilities
- Changelog entries specifically for AI improvements
- Community showcase of creative AI uses

---

## 7. Ethical AI Design Principles

### Transparency
- Disclose when content is AI-generated
- Be honest about AI capabilities and limitations
- Don't use AI to manipulate user behavior (dark patterns)

### Fairness
- Test AI outputs across diverse user groups
- Provide bias detection and reporting mechanisms
- Allow users to flag perceived bias

### Privacy
- Clearly communicate what data AI uses and retains
- Offer opt-out from AI data collection
- Data used for AI training should be anonymized and consented

### Accountability
- Maintain human oversight for high-stakes AI decisions
- Provide audit trails for AI-driven actions
- Have clear escalation paths when AI causes issues

---

## AI Usability Heuristic Checklist

Use this checklist to evaluate any AI feature:

- [ ] **Visibility:** Is AI-generated content clearly distinguishable?
- [ ] **Control:** Can users undo, cancel, and provide feedback?
- [ ] **Explainability:** Can users understand why AI made a recommendation?
- [ ] **Confidence:** Does the AI communicate its certainty level?
- [ ] **Alternatives:** Does the AI present options, not just one answer?
- [ ] **Graceful degradation:** What happens when AI fails? Is there a manual path?
- [ ] **Calibration:** Does the system help users develop appropriate trust?
- [ ] **Privacy:** Is data usage transparent and controllable?
- [ ] **Bias awareness:** Are potential biases acknowledged and mitigatable?
- [ ] **Human override:** Can a human always override the AI's decision?
